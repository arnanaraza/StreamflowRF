pDir <- "E:/GAMMA MAPS/ESACCI-BIOMASS-L4-AGB-MERGED-100m-2018-fv1.0"
rDir <- "E:/Texture2018"
tiles <- list.files(pDir, pattern='*.tif')
discard <- unique (grep(paste(c('err','aux', 'SD','1000m'),collapse="|"),
tiles, value=T))
tiles <- setdiff(tiles,discard)
discard <- list.files(rDir, pattern='*.tif')
chk <- as.data.frame(tiles)
chk$nangina <- chk$tiles
chk1 <- as.data.frame(discard)
chk1$nangina <- chk1$discard
chk2 <- full_join(chk, chk1,  by=c('tiles'='discard'))
which(is.na(chk2$nangina.y))
tiles[160]
tiles[200]
tiles[210]
tiles[240]
tiles[230]
tiles[238]
tiles[232]
tiles[233]
tiles[234]
tiles[235]
tiles[245]
tiles[265]
tiles[255]
tiles[246]
tiles[249]
tiles[251]
tiles[252]
rast<- raster(volcano)
library(raster)
rast<- raster(volcano)
plot(rast)
textures<- glcm(rast, min_x=94.0,max_x=195.0, n_grey=8)
library(glcm)
textures<- glcm(rast, min_x=94.0,max_x=195.0, n_grey=8)
plot(textures$glcm_mean)
textures<- glcm(rast, min_x=94.0,max_x=195.0, n_grey=32)
plot(textures$glcm_mean)
textures<- glcm(rast, min_x=94.0,max_x=195.0, n_grey=32)
plot(textures$glcm_mean)
plot(rast)
textures
rast
textures<- glcm(rast, min_x=NA,max_x=195.0, n_grey=32)
textures<- glcm(rast, min_x=-99,max_x=195.0, n_grey=32)
plot(textures$glcm_mean)
textures<- glcm(rast, min_x=-99,max_x=195.0, n_grey=32,na_val = 1 )
plot(textures$glcm_mean)
textures<- glcm(rast, min_x=-99,max_x=195.0, n_grey=32,na_opt = 'any')
plot(textures$glcm_mean)
plot(rast)
textures<- glcm(rast, min_x=-99,max_x=195.0, n_grey=32,na_opt = 'center')
plot(textures$glcm_mean)
textures<- glcm(rast, min_x=-99,max_x=195.0, n_grey=32,na_opt = 'ignore')
plot(textures$glcm_mean)
plot(rast)
?glcm
textures<- glcm(rast, min_x=-99,max_x=195.0, n_grey=32,na_opt = 'ignore',na_val=NA)
plot(textures$glcm_mean)
textures<- glcm(rast, min_x=-99,max_x=195.0, n_grey=32,na_opt = 'ignore',na_val=99)
plot(textures$glcm_mean)
textures<- glcm(rast, min_x=100,max_x=195.0, n_grey=32)#,na_opt = 'ignore',na_val=99)
plot(textures$glcm_mean)
plot(rast)
textures<- glcm(rast,window=c(15,15) min_x=100,max_x=195.0, n_grey=32)#,na_opt = 'ignore',na_val=99)
textures<- glcm(rast,window=c(15,15), min_x=100,max_x=195.0, n_grey=32)#,na_opt = 'ignore',na_val=99)
plot(textures$glcm_mean)
gc()
# 0. PRELIMS
rm(list = ls())
pacman::p_load(caTools, lubridate, rfUtilities, ranger, hydroGOF,factoextra,corrplot,
rgdal,raster, plyr, dplyr , data.table, reshape2, varhandle, xts,EcoHydRology,
Hmisc, splitstackshape,ggplot2, ggpmisc, caret, gridExtra,ggpubr,Rmisc)
mainDir <- 'D:/StreamflowRF'
dataDir <- 'D:/StreamflowRF/data'
vtDir <- 'D:/StreamflowRF_Results/intermediate/VT/'
predDir <- 'D:/StreamflowRF_Results/results/'
SW.list <- c('aarb_a', 'aarb_n', 'abrb_s','arb_b', 'arb_c', 'crb_a', 'crb_be',
'crb_bu', 'crb_d', 'crb_j','crb_m', 'crb_p', 'crb_s',
'crb_t', 'crb_u', 'mrb_s', 'prb_a', 'prb_b', 'prb_c',
'prb_p', 'prb_r')
setwd(mainDir)
#Open valuetables
all.VT <- read.csv("D:/StreamflowRF_Results/intermediate/VT/all.csv")
pca1.VT <- read.csv("D:/StreamflowRF_Results/intermediate/VT/pca1.csv")
pca2.VT <- read.csv("D:/StreamflowRF_Results/intermediate/VT/pca2.csv")
pca3.VT <- read.csv("D:/StreamflowRF_Results/intermediate/VT/pca3.csv")
pca4.VT <- read.csv("D:/StreamflowRF_Results/intermediate/VT/pca4.csv")
aarb.VT <- read.csv("D:/StreamflowRF_Results/intermediate/VT/aarb.csv")
abrb.VT <- read.csv("D:/StreamflowRF_Results/intermediate/VT/abrb.csv")
arb.VT <- read.csv("D:/StreamflowRF_Results/intermediate/VT/arb.csv")
crb.VT <- read.csv("D:/StreamflowRF_Results/intermediate/VT/crb.csv")
mrb.VT <- read.csv("D:/StreamflowRF_Results/intermediate/VT/mrb.csv")
prb.VT <- read.csv("D:/StreamflowRF_Results/intermediate/VT/prb.csv")
each.VT <- lapply(SW.list, function(x) read.csv(paste0("D:/StreamflowRF_Results/intermediate/VT/",x, '.csv')))
### Model testing (held-out data)
setwd(mainDir)
source('R/predTest.R')
predDir <-"D:/StreamflowRF_Results/results/validation_bc80"
#predDir <-"D:/StreamflowRF_Results/results/validation_LOOCV"
td=0.6
df=all.VT
basin='mrb_s'
RFS='all'
df0 <- df
#prepare training-test data
set.seed(123)
#prepare train-test data
train <- subset(df, df$C.bname != basin)
train.b <- subset(df, df$C.bname == basin)
samp <- floor((1-td)*nrow(train.b))
train.b <- train.b[-train_ind,]
train_ind <- sample(seq_len(nrow(train.b)), size = samp)
train.b <- train.b[-train_ind,]
train <- rbind(train,train.b)
train.b <- train.b[-train_ind,] #
train <- rbind(train,train.b)
train <- train[ , !(names(train) %in% c('date','C.bname'))]
train[] <- sapply(train, as.numeric)
#test <- setdiff( subset(df, df$C.bname == basin), train.b)
test <- train_ind
date <- test$date
train.b <- subset(df, df$C.bname == basin)
samp <- floor((1-td)*nrow(train.b))
train_ind <- sample(seq_len(nrow(train.b)), size = samp)
train.b <- train.b[-train_ind,] #
train <- rbind(train,train.b)
#prepare train-test data
train <- subset(df, df$C.bname != basin)
train.b <- subset(df, df$C.bname == basin)
samp <- floor((1-td)*nrow(train.b))
train_ind <- sample(seq_len(nrow(train.b)), size = samp)
train.b <- train.b[-train_ind,] #
train <- rbind(train,train.b)
train <- train[ , !(names(train) %in% c('date','C.bname'))]
train[] <- sapply(train, as.numeric)
#prepare train-test data
train <- subset(df, df$C.bname != basin)
train.b <- subset(df, df$C.bname == basin)
# Function to predict using RF model then
predTest <- function (td=0.6, df=mrb.VT.yes, basin='mrb_s', RFS='all',predDir) {
df0 <- df
#prepare training-test data
set.seed(123)
if (td != 1){
#prepare train-test data
train <- subset(df, df$C.bname != basin)
train.b <- subset(df, df$C.bname == basin)
samp <- floor((1-td)*nrow(train.b))
train_ind <- sample(seq_len(nrow(train.b)), size = samp)
train.b1 <- train.b[-train_ind,] #
train <- rbind(train,train.b1)
train <- train[ , !(names(train) %in% c('date','C.bname'))]
train[] <- sapply(train, as.numeric)
#test <- setdiff( subset(df, df$C.bname == basin), train.b)
test <- train.b[train_ind,]
date <- test$date
test <- test[ , !(names(test) %in% c('date','C.bname'))]
test[] <- sapply(test, as.numeric)
}else{
df <- df[ , !(names(df) %in% 'date')]
#prepare train-test data
train <- subset(df, df$C.bname != basin)
train <- train[ , !(names(train) %in% 'C.bname')]
train[] <- sapply(train, as.numeric)
test <- subset(df, df$C.bname == basin)
test <- test[ , !(names(test) %in% 'C.bname')]
test[] <- sapply(test, as.numeric)
}
#model training and testing including bias correction
RF <- ranger(O.obs ~ ., data=train, importance='permutation', mtry=15, keep.inbag=T) #should be re-run
val.rf <- predict(RF, test[-17], type='se')
train$O.obs <-  RF$predictions - train$O.obs #bias
rm(RF)
RF1 <-  ranger(O.obs ~ ., data=train, importance='permutation',mtry=15, keep.inbag = T)
print(RF1)
val.rf.bc <- predict(RF1, test[-17], type='se')
rm(RF1)
val.rf$predictions.bc <- val.rf$predictions - val.rf.bc$predictions
#val.rf$predictions.bc <- 2*val.rf$predictions - val.rf.bc$predictions
val.rf$predictions.bc <- ifelse(val.rf$predictions.bc < 0 ,0, val.rf$predictions.bc)
# val.rf$se.bc <- val.rf$se - (val.rf.bc$se - val.rf$se)
val.rf$se.bc <- val.rf$se - val.rf.bc$se
val.rf$se.bc <- ifelse(val.rf$se.bc < 0 ,val.rf$se, val.rf$se.bc)
val.join <- as.data.frame(cbind(test[[17]],val.rf$predictions.bc,
val.rf$predictions,val.rf$se.bc, val.rf$se))
#date <- subset(df0, df0$C.bname == basin)$date ###!
val.join <- cbind(val.join, as.Date(date))
#plot regression graphs
colnames (val.join) <- c('observed','predicted_bc','predicted','se_bc', 'se', 'date')
setwd(predDir)
write.csv(val.join, paste0(basin,'_',RFS,'_',td,'_pred.csv'), row.names=F)
setwd(mainDir)
return(val.join)
gc()
}
#prepare train-test data
train <- subset(df, df$C.bname != basin)
train.b <- subset(df, df$C.bname == basin)
samp <- floor((1-td)*nrow(train.b))
train_ind <- sample(seq_len(nrow(train.b)), size = samp)
train.b1 <- train.b[-train_ind,] #
train <- rbind(train,train.b1)
train <- train[ , !(names(train) %in% c('date','C.bname'))]
train[] <- sapply(train, as.numeric)
#test <- setdiff( subset(df, df$C.bname == basin), train.b)
test <- train.b[train_ind,]
date <- test$date
test <- test[ , !(names(test) %in% c('date','C.bname'))]
test[] <- sapply(test, as.numeric)
#model training and testing including bias correction
RF <- ranger(O.obs ~ ., data=train, importance='permutation', mtry=15, keep.inbag=T) #should be re-run
RF
val.rf <- predict(RF, test[-17]
, type='se')
hist(val.rf$predictions)
hist(val.rf$se)
mean(val.rf$predictions)
val.rf <- predict(RF, test[-17])#, type='se')
mean(val.rf$predictions)
train$O.obs <-  RF$predictions - train$O.obs #bias
hist(train$O.obs )
rm(RF)
RF1 <-  ranger(O.obs ~ ., data=train, importance='permutation',mtry=15, keep.inbag = T)
print(RF1)
val.rf.bc <- predict(RF1, test[-17], type='se')
rm(RF1)
val.rf$predictions.bc <- val.rf$predictions - val.rf.bc$predictions
hist(val.rf$predictions.bc)
val.rf$predictions.bc <- val.rf$predictions - val.rf.bc$predictions
#val.rf$predictions.bc <- 2*val.rf$predictions - val.rf.bc$predictions
val.rf$predictions.bc <- ifelse(val.rf$predictions.bc < 0 ,0, val.rf$predictions.bc)
# val.rf$se.bc <- val.rf$se - (val.rf.bc$se - val.rf$se)
val.rf$se.bc <- val.rf$se - val.rf.bc$se
val.rf$se.bc <- ifelse(val.rf$se.bc < 0 ,val.rf$se, val.rf$se.bc)
val.join <- as.data.frame(cbind(test[[17]],val.rf$predictions.bc,
val.rf$predictions,val.rf$se.bc, val.rf$se))
#date <- subset(df0, df0$C.bname == basin)$date ###!
val.join <- cbind(val.join, as.Date(date))
#plot regression graphs
colnames (val.join) <- c('observed','predicted_bc','predicted','se_bc', 'se', 'date')
head(val.join)
val.join <- as.data.frame(cbind(test[[17]],val.rf$predictions.bc,
val.rf$predictions,val.rf$se.bc, val.rf$se))
#date <- subset(df0, df0$C.bname == basin)$date ###!
val.join <- cbind(val.join, as.Date(date))
#plot regression graphs
colnames (val.join) <- c('observed','predicted_bc','predicted','se_bc', 'se', 'date')
head(val.join)
val.rf$predictions.bc <- val.rf$predictions - val.rf.bc$predictions
#val.rf$predictions.bc <- 2*val.rf$predictions - val.rf.bc$predictions
val.rf$predictions.bc <- ifelse(val.rf$predictions.bc < 0 ,0, val.rf$predictions.bc)
# val.rf$se.bc <- val.rf$se - (val.rf.bc$se - val.rf$se)
val.rf$se.bc <- val.rf$se - val.rf.bc$se
val.rf$se.bc <- ifelse(val.rf$se.bc < 0 ,val.rf$se, val.rf$se.bc)
val.join <- as.data.frame(cbind(test[[17]],val.rf$predictions.bc,
val.rf$predictions,val.rf$se.bc, val.rf$se))
#date <- subset(df0, df0$C.bname == basin)$date ###!
val.join <- cbind(val.join, as.Date(date))
#plot regression graphs
names (val.join) <- c('observed','predicted_bc','predicted','se_bc', 'se', 'date')
names(val.join)
hist(val.rf$predictions.bc, )
hist(val.rf$predictions)
hist(val.rf$se.bc)
hist(val.rf$se.bc)
hist(val.rf$se.bc[[1]])
View(val.rf.bc)
val.rf <- predict(RF, test[-17], type='se')
#model training and testing including bias correction
RF <- ranger(O.obs ~ ., data=train, importance='permutation', mtry=15, keep.inbag=T) #should be re-run
val.rf <- predict(RF, test[-17], type='se')
train$O.obs <-  RF$predictions - train$O.obs #bias
RF1 <-  ranger(O.obs ~ ., data=train, importance='permutation',mtry=15, keep.inbag = T)
print(RF1)
val.rf.bc <- predict(RF1, test[-17], type='se')
View(train)
unique(train$C.basin)
unique(train$C.area)
876+1315
val.rf$predictions.bc <- val.rf$predictions - val.rf.bc$predictions
#val.rf$predictions.bc <- 2*val.rf$predictions - val.rf.bc$predictions
val.rf$predictions.bc <- ifelse(val.rf$predictions.bc < 0 ,0, val.rf$predictions.bc)
nrow(val.rf$predictions.bc)
length(val.rf$predictions.bc)
val.join <- data.frame(cbind(test[[17]],val.rf$predictions.bc,
val.rf$predictions,val.rf$se.bc, val.rf$se))
# val.rf$se.bc <- val.rf$se - (val.rf.bc$se - val.rf$se)
val.rf$se.bc <- val.rf$se - val.rf.bc$se
val.rf$se.bc <- ifelse(val.rf$se.bc < 0 ,val.rf$se, val.rf$se.bc)
val.join <- data.frame(cbind(test$O.obs,val.rf$predictions.bc,
val.rf$predictions,val.rf$se.bc, val.rf$se))
#date <- subset(df0, df0$C.bname == basin)$date ###!
val.join <- cbind(val.join, as.Date(date))
#plot regression graphs
names (val.join) <- c('observed','predicted_bc','predicted','se_bc', 'se', 'date')
setwd(predDir)
write.csv(val.join, paste0(basin,'_',RFS,'_',td,'_pred.csv'), row.names=F)
setwd(mainDir)
return(val.join)
val.join
# Function to predict using RF model then
predTest <- function (td=0.6, df=mrb.VT.yes, basin='mrb_s', RFS='all',predDir) {
df0 <- df
#prepare training-test data
set.seed(123)
if (td != 1){
#prepare train-test data
train <- subset(df, df$C.bname != basin)
train.b <- subset(df, df$C.bname == basin)
samp <- floor((1-td)*nrow(train.b))
train_ind <- sample(seq_len(nrow(train.b)), size = samp)
train.b1 <- train.b[-train_ind,] #
train <- rbind(train,train.b1)
train <- train[ , !(names(train) %in% c('date','C.bname'))]
train[] <- sapply(train, as.numeric)
#test <- setdiff( subset(df, df$C.bname == basin), train.b)
test <- train.b[train_ind,]
date <- test$date
test <- test[ , !(names(test) %in% c('date','C.bname'))]
test[] <- sapply(test, as.numeric)
}else{
df <- df[ , !(names(df) %in% 'date')]
#prepare train-test data
train <- subset(df, df$C.bname != basin)
train <- train[ , !(names(train) %in% 'C.bname')]
train[] <- sapply(train, as.numeric)
test <- subset(df, df$C.bname == basin)
test <- test[ , !(names(test) %in% 'C.bname')]
test[] <- sapply(test, as.numeric)
}
#model training and testing including bias correction
RF <- ranger(O.obs ~ ., data=train, importance='permutation', mtry=15, keep.inbag=T) #should be re-run
val.rf <- predict(RF, test[-17], type='se')
train$O.obs <-  RF$predictions - train$O.obs #bias
RF1 <-  ranger(O.obs ~ ., data=train, importance='permutation',mtry=15, keep.inbag = T)
print(RF1)
val.rf.bc <- predict(RF1, test[-17], type='se')
val.rf$predictions.bc <- val.rf$predictions - val.rf.bc$predictions
#val.rf$predictions.bc <- 2*val.rf$predictions - val.rf.bc$predictions
val.rf$predictions.bc <- ifelse(val.rf$predictions.bc < 0 ,0, val.rf$predictions.bc)
# val.rf$se.bc <- val.rf$se - (val.rf.bc$se - val.rf$se)
val.rf$se.bc <- val.rf$se - val.rf.bc$se
val.rf$se.bc <- ifelse(val.rf$se.bc < 0 ,val.rf$se, val.rf$se.bc)
val.join <- data.frame(cbind(test$O.obs,val.rf$predictions.bc,
val.rf$predictions,val.rf$se.bc, val.rf$se))
#date <- subset(df0, df0$C.bname == basin)$date ###!
val.join <- cbind(val.join, as.Date(date))
#plot regression graphs
names (val.join) <- c('observed','predicted_bc','predicted','se_bc', 'se', 'date')
setwd(predDir)
write.csv(val.join, paste0(basin,'_',RFS,'_',td,'_pred.csv'), row.names=F)
setwd(mainDir)
return(val.join)
gc()
}
source('R/predTest.R')
predDir <-"D:/StreamflowRF_Results/results/validation_bc80"
#predDir <-"D:/StreamflowRF_Results/results/validation_LOOCV"
start_time <- Sys.time()
lapply(unique(all.VT$C.bname), function(x) predTest(0.8, all.VT, x, 'all', predDir))
lapply(unique(pca1.VT$C.bname), function(x) predTest(0.8, pca1.VT,x, 'pca', predDir))
lapply(unique(pca2.VT$C.bname), function(x) predTest(0.8, pca2.VT,x, 'pca', predDir))
lapply(unique(pca3.VT$C.bname), function(x) predTest(0.8, pca3.VT,x, 'pca', predDir))
lapply(unique(pca4.VT$C.bname), function(x) predTest(0.8, pca4.VT,x, 'pca', predDir))
lapply(unique(aarb.VT$C.bname), function(x) predTest(0.8, aarb.VT,x, 'basin', predDir))
lapply(unique(abrb.VT$C.bname), function(x) predTest(0.8, abrb.VT,x, 'basin', predDir))
lapply(unique(arb.VT$C.bname), function(x) predTest(0.8, arb.VT,x, 'basin', predDir))
lapply(unique(crb.VT$C.bname), function(x) predTest(0.8, crb.VT,x, 'basin', predDir))
lapply(unique(mrb.VT$C.bname), function(x) predTest(0.8, mrb.VT,x, 'basin', predDir))
lapply(unique(prb.VT$C.bname), function(x) predTest(0.8, prb.VT,x, 'basin', predDir))
lapply(1:21, function(x) predTest(0.8,each.VT[[x]], SW.list[[x]], 'shed',predDir))
end_time <- Sys.time()
end_time - start_time
### ACCURACY
setwd(mainDir)
source('R/Acc.R')
setwd(predDir)
pred.list <- list.files(predDir,pattern=glob2rx('*all*0.8*'))
all.df <- ldply(lapply(1:length(pred.list), function(x) Acc(read.csv(pred.list[[x]]),SW.list[[x]])), data.frame)
all.df$method <- 'all'
pred.list <- list.files(predDir,pattern=glob2rx('*pca*0.8*'))
SW.list <-gsub("(_[a-z]).*","\\1",pred.list)
SW.list[7:8] <- c('crb_be', 'crb_bu')
pca.df <- ldply(lapply(1:length(pred.list), function(x) Acc(read.csv(pred.list[[x]]),SW.list[[x]])), data.frame)
pca.df$method <- 'pca'
pred.list <- list.files(predDir,pattern=glob2rx('*basin*0.8*'))
SW.list <-gsub("(_[a-z]).*","\\1",pred.list)
SW.list[6:7] <- c('crb_be', 'crb_bu')
basin.df <- ldply(lapply(1:length(pred.list), function(x) Acc(read.csv(pred.list[[x]]),SW.list[[x]])), data.frame)
basin.df$method <- 'basin'
pred.list <- list.files(predDir,pattern=glob2rx('*shed*0.8*'))
SW.list <-gsub("(_[a-z]).*","\\1",pred.list)
shed.df <- ldply(lapply(1:length(pred.list), function(x) Acc(read.csv(pred.list[[x]]),SW.list[[x]])), data.frame)
shed.df$method <- 'shed'
all.pred <- left_join(all.df, pca.df, by = c('str'='str'))
all.pred <- left_join(all.pred, basin.df, by = c('str'='str'))
all.pred <- data.frame(all.df, pca.df,basin.df, shed.df)
write.csv(all.pred, 'acc_80_bc.csv', row.names=F)
td
td=0.8
Rf
importance(RF)
RF
RF1
RF
# Function to predict using RF model then
predTest <- function (td=0.6, df=mrb.VT.yes, basin='mrb_s', RFS='all',predDir) {
df0 <- df
#prepare training-test data
#set.seed(123)
if (td != 1){
#prepare train-test data
train <- subset(df, df$C.bname != basin)
train.b <- subset(df, df$C.bname == basin)
samp <- floor((1-td)*nrow(train.b))
train_ind <- sample(seq_len(nrow(train.b)), size = samp)
train.b1 <- train.b[-train_ind,] #
train <- rbind(train,train.b1)
train <- train[ , !(names(train) %in% c('date','C.bname'))]
train[] <- sapply(train, as.numeric)
#test <- setdiff( subset(df, df$C.bname == basin), train.b)
test <- train.b[train_ind,]
date <- test$date
test <- test[ , !(names(test) %in% c('date','C.bname'))]
test[] <- sapply(test, as.numeric)
}else{
df <- df[ , !(names(df) %in% 'date')]
#prepare train-test data
train <- subset(df, df$C.bname != basin)
train <- train[ , !(names(train) %in% 'C.bname')]
train[] <- sapply(train, as.numeric)
test <- subset(df, df$C.bname == basin)
test <- test[ , !(names(test) %in% 'C.bname')]
test[] <- sapply(test, as.numeric)
}
#model training and testing including bias correction
RF <- ranger(O.obs ~ ., data=train, importance='permutation', mtry=15, keep.inbag=T) #should be re-run
print(importance(RF))
val.rf <- predict(RF, test[-17], type='se')
train$O.obs <-  RF$predictions - train$O.obs #bias
RF1 <-  ranger(O.obs ~ ., data=train, importance='permutation',mtry=15, keep.inbag = T)
val.rf.bc <- predict(RF1, test[-17], type='se')
val.rf$predictions.bc <- val.rf$predictions - val.rf.bc$predictions
#val.rf$predictions.bc <- 2*val.rf$predictions - val.rf.bc$predictions
val.rf$predictions.bc <- ifelse(val.rf$predictions.bc < 0 ,0, val.rf$predictions.bc)
# val.rf$se.bc <- val.rf$se - (val.rf.bc$se - val.rf$se)
val.rf$se.bc <- val.rf$se - val.rf.bc$se
val.rf$se.bc <- ifelse(val.rf$se.bc < 0 ,val.rf$se, val.rf$se.bc)
val.join <- data.frame(cbind(test$O.obs,val.rf$predictions.bc,
val.rf$predictions,val.rf$se.bc, val.rf$se))
#date <- subset(df0, df0$C.bname == basin)$date ###!
val.join <- cbind(val.join, as.Date(date))
#plot regression graphs
names (val.join) <- c('observed','predicted_bc','predicted','se_bc', 'se', 'date')
setwd(predDir)
write.csv(val.join, paste0(basin,'_',RFS,'_',td,'_pred.csv'), row.names=F)
setwd(mainDir)
return(val.join)
gc()
}
### Model testing (held-out data)
setwd(mainDir)
source('R/predTest.R')
predDir <-"D:/StreamflowRF_Results/results/validation_bc50"
start_time <- Sys.time()
lapply(unique(all.VT$C.bname), function(x) predTest(0.5, all.VT, x, 'all', predDir))
lapply(unique(pca1.VT$C.bname), function(x) predTest(0.5, pca1.VT,x, 'pca', predDir))
lapply(unique(pca2.VT$C.bname), function(x) predTest(0.5, pca2.VT,x, 'pca', predDir))
lapply(unique(pca3.VT$C.bname), function(x) predTest(0.5, pca3.VT,x, 'pca', predDir))
lapply(unique(pca4.VT$C.bname), function(x) predTest(0.5, pca4.VT,x, 'pca', predDir))
lapply(unique(aarb.VT$C.bname), function(x) predTest(0.5, aarb.VT,x, 'basin', predDir))
lapply(unique(abrb.VT$C.bname), function(x) predTest(0.5, abrb.VT,x, 'basin', predDir))
lapply(unique(arb.VT$C.bname), function(x) predTest(0.5, arb.VT,x, 'basin', predDir))
lapply(unique(crb.VT$C.bname), function(x) predTest(0.5, crb.VT,x, 'basin', predDir))
lapply(unique(mrb.VT$C.bname), function(x) predTest(0.5, mrb.VT,x, 'basin', predDir))
lapply(unique(prb.VT$C.bname), function(x) predTest(0.5, prb.VT,x, 'basin', predDir))
lapply(1:21, function(x) predTest(0.5,each.VT[[x]], SW.list[[x]], 'shed',predDir))
end_time <- Sys.time()
end_time - start_time
gc()
### ACCURACY
setwd(mainDir)
source('R/Acc.R')
setwd(predDir)
pred.list <- list.files(predDir,pattern=glob2rx('*all*0.5*'))
all.df <- ldply(lapply(1:length(pred.list), function(x) Acc(read.csv(pred.list[[x]]),SW.list[[x]])), data.frame)
all.df$method <- 'all'
pred.list <- list.files(predDir,pattern=glob2rx('*pca*0.5*'))
SW.list <-gsub("(_[a-z]).*","\\1",pred.list)
SW.list[7:8] <- c('crb_be', 'crb_bu')
pca.df <- ldply(lapply(1:length(pred.list), function(x) Acc(read.csv(pred.list[[x]]),SW.list[[x]])), data.frame)
pca.df$method <- 'pca'
pred.list <- list.files(predDir,pattern=glob2rx('*basin*0.5*'))
SW.list <-gsub("(_[a-z]).*","\\1",pred.list)
SW.list[6:7] <- c('crb_be', 'crb_bu')
basin.df <- ldply(lapply(1:length(pred.list), function(x) Acc(read.csv(pred.list[[x]]),SW.list[[x]])), data.frame)
basin.df$method <- 'basin'
pred.list <- list.files(predDir,pattern=glob2rx('*shed*0.5*'))
SW.list <-gsub("(_[a-z]).*","\\1",pred.list)
shed.df <- ldply(lapply(1:length(pred.list), function(x) Acc(read.csv(pred.list[[x]]),SW.list[[x]])), data.frame)
shed.df$method <- 'shed'
all.pred <- left_join(all.df, pca.df, by = c('str'='str'))
all.pred <- left_join(all.pred, basin.df, by = c('str'='str'))
all.pred <- data.frame(all.df, pca.df,basin.df, shed.df)
write.csv(all.pred, 'acc_50_bc.csv', row.names=F)
### Model testing (held-out data)
setwd(mainDir)
source('R/predTest.R')
predDir <-"D:/StreamflowRF_Results/results/validation_bc50"
